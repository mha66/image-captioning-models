# ğŸ–¼ï¸ AI Image Captioning
## Visionâ€“Language Models with Streamlit Demo

<img style="width: 80%; height: auto;" alt="image" src="https://github.com/user-attachments/assets/2f139bd2-69b3-4091-bf28-bf19b80c9259" />
<img style="width: 90%; height: auto;" alt="image" src="https://github.com/user-attachments/assets/a23ff623-2c43-4341-a207-f3758fedbf34" />

---

This project implements an **Image Captioning system** using multiple **Encoderâ€“Decoder (Visionâ€“Language)** architectures.  
Given an input image, the model generates a **natural language caption** describing the image content.

The project also includes an **interactive Streamlit web application** that allows users to upload images and compare captions generated by different models.

---

## ğŸ”— Links

<a href="https://colab.research.google.com/drive/1U1kQDdalv-pzU_j9sMcFpuux745qyqBa?usp=sharing">Link to Colab Notebook</a><br><br>
**Checkpoints:**
* <a href="https://www.kaggle.com/datasets/mohish66/image-captioning-ckeckpoints">ViT + GPT-2 Checkpoints</a>
* <a href="https://www.kaggle.com/datasets/mohish66/image-captioning-vit-distilgpt2">ViT + DistilGPT-2 Checkpoints</a>
* <a href="https://www.kaggle.com/datasets/mohish66/image-captioning-clip-gpt2">CLIP + GPT-2 Checkpoints</a>

---

## ğŸš€ Project Overview

- ğŸ” **Task:** Image Captioning  
- ğŸ§  **Architecture:** Encoderâ€“Decoder with Cross-Attention  
- ğŸ–¼ï¸ **Vision Encoders:** ViT, CLIP  
- ğŸ“ **Text Decoders:** GPT-2, DistilGPT-2  
- ğŸŒ **Interface:** Streamlit Web App  
- ğŸ“Š **Evaluation Metric:** BLEU Score  

---

## ğŸ§  Models Used

This project compares **three different visionâ€“language model combinations**:

### 1ï¸âƒ£ ViT + GPT-2
- **Encoder:** `google/vit-base-patch16-224`
- **Decoder:** `gpt2`
- Strong language generation but higher computational cost.

### 2ï¸âƒ£ ViT + DistilGPT-2 â­ (Best Overall)
- **Encoder:** `google/vit-base-patch16-224`
- **Decoder:** `distilgpt2`
- Best balance between:
  - caption quality
  - generalization
  - speed and efficiency

### 3ï¸âƒ£ CLIP + GPT-2
- **Encoder:** `openai/clip-vit-base-patch32`
- **Decoder:** `gpt2`
- Benefits from CLIPâ€™s imageâ€“text pretraining but is more computationally expensive.

---

## ğŸ† Best Model Selection

Although **CLIP + GPT-2** achieved strong training performance,  
**ViT + DistilGPT-2** was selected as the **best overall model** due to:

- Better generalization on unseen images  
- Faster inference  
- Lower memory and computational requirements  
- More stable caption generation  

---

## ğŸ”§ Architecture

```
Image
  â†“
Vision Encoder (ViT / CLIP)
  â†“
Projection Layer (Linear)
  â†“
GPT-style Decoder (GPT-2 / DistilGPT-2) + Cross-Attention
  â†“
Generated Caption (word by word)
```

- The **projection layer** aligns encoder and decoder embedding sizes.
- The decoder generates captions **autoregressively (token by token)**.

---

## ğŸ” Caption Generation (Autoregressive)

Captions are generated **one word at a time**:

1. Start with `<BOS>` token  
2. Predict the next word using:
   - previously generated words
   - image embeddings (via cross-attention)  
3. Append the word and repeat  
4. Stop at `<EOS>` or maximum length  

This ensures:
- correct word order  
- grammatical consistency  
- semantic coherence  

---

## ğŸ–¥ï¸ Streamlit Web Application

The project includes an interactive **Streamlit UI** that allows:

- Uploading images (`JPG`, `JPEG`, `PNG`)
- Selecting a model from a dropdown
- Viewing the generated caption instantly
- Displaying the selected encoder and decoder

### Supported Models in the App

| Model | Encoder | Decoder |
|-----|--------|--------|
| ViT + GPT-2 | ViT | GPT-2 |
| **ViT + DistilGPT-2** | ViT | DistilGPT-2 |
| CLIP + GPT-2 | CLIP | GPT-2 |

---

## â–¶ï¸ Run the Streamlit App
1. Download the three models' checkpoints from the provided links
2. Create a folder named `models` inside the project directory
3. Move the downloaded checkpoints to the `models` folder
4. Rename the checkpoints as follows:
   *  `checkpoint_epoch_6.pth.zip` to `vit_gpt2.pth.zip`
   *  `distilgpt2_epoch_6.pth.zip` to `vit_distilgpt2.pth.zip`
   *  `clip_gpt2_epoch_14.pth.zip` to `clip_gpt2.pth.zip`

5. Run the following command in a terminal:
```bash
streamlit run app.py
```

6. Then open:
```
http://localhost:8501
```

## ğŸ“Š Datasets

### Flickr8k
- Used for training
- 8,000 images with multiple captions per image

### Flickr30k
- Used for testing
- Filtered to remove images seen during training

---

## ğŸ“ˆ Evaluation

- **Metric:** BLEU Score  
- Measures similarity between generated captions and human annotations
- Considers:
  - n-gram overlap
  - brevity penalty  

---

## ğŸ› ï¸ Installation
**install dependencies**
```bash
pip install -r requirements.txt
```

## ğŸ“œ License

This project is for **educational and research purposes**.
